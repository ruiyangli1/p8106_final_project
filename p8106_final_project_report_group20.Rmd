---
title: "Predicting Crime Rate Within Boston Area"
author: "Group 20"
date: "5/13/2021"
output: 
  pdf_document: 
    latex_engine: xelatex
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE)

library(tidyverse)
library(gtsummary)
library(summarytools)
library(kableExtra)
library(caret)
library(corrplot)
library(patchwork)
library(rpart.plot)    # for visualization, tree diagrams
library(vip)
library(pdp)
library(lime)

theme_set(theme_bw())
```



# Introduction

## Motivation 

Public safety has always been one of the most concerns in our daily lives. A safe environment with a low crime rate not only protects us from injury and illness but also helps us improve our overall quality of life. Further, reducing crimes can lead to a decrease in societal costs and, as a result, generate substantial economic benefits. Therefore, in this project, we would like to identify the influential factors that could potentially predict the crime rate and hopefully provide insight for future policy to improve population safety. Specifically, we focus on the per capita crime rate within the Boston area. The overall goal of this project is to predict the crime rate and identify the important predicting variables as well as understand their impact on population safety.

## Data

The dataset that we used in this project can be assessed from the StatLib library which is maintained at Carnegie Mellon University. It contains information about 14 variables that could potentially explain crime rate from 506 observations. 13 out of 14 variables are continuous variables, and the variable CHAS is a dichotomous variable, which has been converted to the factor type. The detailed description of each variables is displayed in Table 1. The outcome of interest is the per capita crime rate (CRIM), and the remaining 13 variables are the predictors. 

This dataset has already been cleaned and does not contain missing or duplicated values. If the variable names are too messy, we can use the `janitor::clean_names()` function to make the variable names unique and consist only of the "_" character, numbers, and letters. If there are a small proportion of missing values, we may consider using r functions such as `na.omit` or `drop_na()` to remove them; otherwise, imputation may be needed. The distribution of each predictor variable can be seen in Figure 1. After careful consideration, we decided not to perform transformations in order to maintain the interpretability of the predictors.

We used `createDataPartition()` function to randomly split our dataset into a training dataset (75%) and a test dataset (25%). We performed model fitting and model selection on the training dataset and evaluated its test performance on the test dataset. 

```{r 1.df}
# Import dataset
data0 = read_csv("./data/housing.csv")

data = data0 %>%
  mutate(CHAS = factor(CHAS,
                       levels = c(0,1),
                       labels = c("tract bounds river","otherwise")))
  

label(data$CRIM) <- "Crime Rate"
label(data$ZN) <- "Zone"
label(data$INDUS) <- "Non-retail Business"
label(data$CHAS) <- "Charles River"
label(data$NOX) <- "Nitric Oxide"
label(data$RM) <- "Room"
label(data$AGE) <- "Age"
label(data$DIS) <- "Distance"
label(data$RAD) <- "Radial Highways"
label(data$TAX) <- "Tax"
label(data$PTRATIO) <- "Pupil/teacher"
label(data$B) <- "Blacks"
label(data$LSTAT) <- "Low Status"
label(data$MEDV) <- "Median home value (in 1k)"
```

```{r 1.var_descpt}
# Variable description

var = data.frame(
  Variable = colnames(data), 
  Description = 
    c('per capita crime rate by town',
      'proportion of residential land zoned for lots over 25,000 sq.ft.',
      'proportion of non-retail business acres per town',
      'Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)',
      'nitrogen oxides concentration (parts per 10 million)',
      'average number of rooms per dwelling',
      'proportion of owner-occupied units built prior to 1940',
      'weighted distances to five Boston employment centres',
      'index of accessibility to radial highways',
      'full-value property-tax rate per $10,000',
      'pupil-teacher ratio by town',
      '1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town',
      '% lower status of the population',
      'Median value of owner-occupied homes in $1000s')) %>% 
  arrange(Variable)
  
var_descpt = 
  kbl(var, booktabs = T) %>% 
  kable_styling(full_width = T,
                latex_options = c("HOLD_position")) %>% 
  column_spec(1, width = "5cm")
```

```{r 1.descpt_stt_(optional)}
# optional
descpt_stt =
   data %>% 
  select(var$Variable) %>% 
  tbl_summary(
    type = all_continuous() ~ "continuous2",
    statistic = all_continuous() ~ c("{mean} ({sd})", "{median} ({p25}, {p75})", "{min}, {max}")) %>% 
  bold_labels() %>% 
  italicize_levels() %>% 
  as_gt()
```

```{r 1.train_test_df}
# Split dataset

set.seed(20210323)
trRows = createDataPartition(data$CRIM, p = .75, list = F)

data_train = data[trRows,]
data_test = data[-trRows,]

x <- model.matrix(CRIM ~ ., data_train)[ ,-1] 
x_test <- model.matrix(CRIM ~ ., data_test)[ ,-1] 

y <- data_train$CRIM
y_test <- data_test$CRIM
```

```{r 1.var_distn}

#dfSummary(data, graph.magnif = 0.75)

# Check for missing values
#sum(is.na(data))
# Check for duplicated values
#sum(duplicated(data))

var_distn = 
  data0[trRows,] %>%
  mutate(CHAS = factor(CHAS,
                       levels = c(0,1),
                       labels = c("tract bounds river","otherwise"))) %>% 
  pivot_longer(-c(CHAS), names_to = "var", values_to = "value") %>% 
  ggplot(aes(x = value, color = CHAS)) +  
  facet_wrap(~ var, scales = "free") +
  geom_density() +
  labs(x = NULL) + 
  scale_color_manual(values = c("#999999", "#E69F00", "#56B4E9")) + 
  theme_bw() 
```


# Exploratory Analysis/Visualization

From the plot of each predictor versus crime rate (Figure 2), we can see that some variables seem to have nonlinear patterns with crime rate whereas others seem to have linear patterns. Therefore, it is reasonable to consider using non-linear models (e.g., GAM and MARS), tree-based methods (e.g., regression tree and conditional inference tree), and ensemble methods (e.g., random forests and boosting) to fit the data in addition to linear models. 

We also examined the multi-collinearity among predictors. This is because if two predictors are highly correlated, it may undermine the statistical significance of the variable. From the correlation plot (Figure 3), we can observe that there are a number of predictors highly correlated with each other. For example, the per capita crime rate (CRIM) seems to have a high positive correlation with the index of accessibility to radial highways (RAD) and full-value property-tax rate per $10,000 (TAX). The weighted distances to five Boston employment centers (DIS) seem to have a high positive correlation with the proportion of residential land zoned for lots over 25,000 sq.ft. (ZN) and a high negative correlation with the proportion of non-retail business acres per town (INDUS), nitrogen oxideS concentration per 10 million (NOX), and proportion of owner-occupied units built prior to 1940 (AGE). The proportion of non-retail business acres per town (INDUS) and % lower status of the population (LSTAT) seem to be correlated with most predictors except for CHAS, the dummy variable indicating whether the tract of the Charles River bounds river. 

We will explore more about the predicting abilities of these variables for crime rate in the following Models section. 

```{r 2.var_vs_crim_plt}
scat_plt = 
  data0[trRows,] %>% 
  select(-CHAS) %>% 
  pivot_longer(
    -CRIM,
    names_to = "var", 
    values_to = "value"
  ) %>%
  ggplot(aes(x = value, y = CRIM)) + 
  geom_point() + 
  stat_smooth(method = "loess", se = T, col = "#E69F00") + 
  facet_wrap(~var, scales = "free") + 
  labs(x = NULL) + 
  theme_bw()

box_plt = 
  data[trRows,] %>% 
  select(CHAS,CRIM) %>%
  ggplot(aes(x = CHAS,y = CRIM, color = CHAS)) +
  geom_boxplot() + 
  facet_wrap(~CHAS, scales = "free") + 
  scale_color_manual(name = "CHAS", 
                    values = c("#999999", "#E69F00")) +
  scale_x_discrete(labels = "") + 
  theme_bw() + 
  theme(legend.position = "none")
```

```{r 2.corplot,include=FALSE}
# check multi-collinearity
corplt = 
  corrplot(cor(data0[trRows,]), method = "circle", type = "full", tl.cex = 0.53)
```


# Models

## Model Determination

There are no missing values among all the variables. We checked through all the variables to see if there is any near zero variance predictor using the `nearZeroVar` function in the datasets before and after splitting, and the result shows there does not exist near zero variance predictors. Therefore, we used crime rate as our response variable and all the remaining variables as the predictors. 

Since crime rate is a continuous outcome, we performed the modeling under the regression framework. Our earlier exploratory findings suggested that there seemed to be some non-linear association between the predictors and the outcome. Therefore, we considered both linear and non-linear models during the modeling process. Specifically, we fitted linear models including multiple linear regression, lasso regression, ridge regression, elastic net, principal component regression (PCR), and partial least squares (PLS) model, and non-linear models such as generalized additive model (GAM), multivariate adaptive regression splines (MARS), regression tree, conditional inference tree, random forests, and boosting. 

```{r 3.models,include=FALSE}

# check near zero variance predictors
# nearZeroVar(data)
# nearZeroVar(x)
# nearZeroVar(x_test)

# Model fitting 

# resampling method
ctrl1 <- trainControl(method = "cv", number = 10)


# --- linear model
set.seed(20210323)
lm.fit <- train(x, y, method = "lm", trControl = ctrl1)

# prediction
lm.pred <- predict(lm.fit, newdata = x_test)
# test error
lm.mse = mean((lm.pred - y_test)^2)



# --- ridge 
set.seed(20210323)
ridge.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0, 
                                          lambda = exp(seq(2,-3,length = 100))),
                   trControl = ctrl1)
# select optimal tuning parameter
#ridge.fit$bestTune$lambda
ridge.tun.plt = plot(ridge.fit, xTrans = log)

# prediction 
ridge.pred <- predict(ridge.fit$finalModel, 
                      newx = x_test, 
                      s = ridge.fit$bestTune$lambda, 
                      type = "response")
# test error
ridge.mse = mean((ridge.pred - y_test)^2)



# --- lasso
set.seed(20210323)
lasso.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(1,-5,length = 100))),
                   trControl = ctrl1)
# select optimal tuning parameter
#lasso.fit$bestTune$lambda
lasso.tun.plt = plot(lasso.fit, xTrans = log)
# prediction 
lasso.pred <- predict(lasso.fit$finalModel, 
                      newx = x_test, 
                      s = lasso.fit$bestTune$lambda, 
                      type = "response")
# test error
lasso.mse = mean((lasso.pred - y_test)^2) 
# coefficients in the final model
#lasso_coef = coef(lasso.fit$finalModel, s = lasso.fit$bestTune$lambda)



# --- Elastic net
set.seed(20210323)
enet.fit <- train(x, y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 5), 
                                         lambda = exp(seq(-10, -3,
                                                          length = 50))),
                  trControl = ctrl1)
# select optimal tuning parameter
#enet.fit$bestTune$lambda
emet.tun.plt = ggplot(enet.fit)

# prediction 
enet.pred <- predict(enet.fit$finalModel, 
                     newx = x_test, 
                     s = enet.fit$bestTune$lambda, 
                     type = "response")
# test error
enet.mse = mean((enet.pred - y_test)^2) 



# --- pcr
set.seed(20210323)
pcr.fit <- train(x, y,
                 method = "pcr",
                 tuneGrid = data.frame(ncomp = 1:13),
                 trControl = ctrl1,
                 scale = TRUE)
# select optimal tuning parameter
pcr.tun.plt = ggplot(pcr.fit, highlight = TRUE)

# predicted y
pcr.pred <- predict(pcr.fit, newdata = x_test)
# test error
pcr.mse = mean((pcr.pred - y_test)^2)



# --- pls
set.seed(20210323)
pls.fit <- train(x, y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:13),
                 trControl = ctrl1,
                 scale = TRUE)
# select optimal tuning parameter
pls.tun.plt = ggplot(pls.fit, highlight = TRUE)

# predicted y
pls.pred <- predict(pls.fit, newdata = x_test)
# test error
pls.mse = mean((pls.pred - y_test)^2)



# --- GAM model
set.seed(20210323)
gam.fit <- train(x, y, 
                 method = "gam",
                 trControl = ctrl1)
#gam.fit$bestTune
#gam.fit$finalModel
#summary(gam.fit)
#plot(gam.fit$finalModel, pages = 2) # plot for s()
# predicted y
#gam.pred <- predict(gam.fit$finalModel, newdata = x_test)
# test error
#gam.mse = mean((gam.pred - y_test)^2)



# --- MARS model
mars_grid <- expand.grid(degree = 1:3, 
                         nprune = 2:22)
set.seed(20210323)
mars.fit <- train(x, y,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)
# select optimal tuning parameter
#mars.fit$bestTune
mars.tun.plt = ggplot(mars.fit)

# predicted y
mars.pred <- predict(mars.fit$finalModel, newdata = x_test, type = "earth")
# test error
mars.mse = mean((mars.pred - y_test)^2)



# --- regression tree
set.seed(20210323)
rpart.fit <- train(CRIM ~ . , 
                   data_train, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-3, length = 50))),
                   trControl = ctrl1)
# select optimal tuning parameter
#rpart.fit$bestTune
rpart.tun.plt = ggplot(rpart.fit, highlight = TRUE)

# tree visualization
rpart.plot(rpart.fit$finalModel)
# predicted y
rpart.pred <- predict(rpart.fit, newdata = data_test)
# test error
rpart.mse = mean((rpart.pred - y_test)^2)



# --- conditional inference tree model
set.seed(20210323)
ctree.fit <- train(CRIM ~ . ,  
                   data_train, 
                   method = "ctree",
                   tuneGrid = data.frame(
                     mincriterion = 1 - exp(seq(-5, -3, length = 50))),
                   trControl = ctrl1)
# select optimal tuning parameter
#ctree.fit$bestTune
ctree.tun.plt = ggplot(ctree.fit, highlight = TRUE)

# tree visualization
plot(ctree.fit$finalModel)
# predicted y
ctree.pred <- predict(ctree.fit, newdata = data_test)
# test error
ctree.mse = mean((ctree.pred - y_test)^2)



# --- random forest (bagging at mtry = 13)
rf.grid <- expand.grid(mtry = 1:13, # #of selected variable at each split
                       splitrule = "variance", # use RSS rule to make the split
                       min.node.size = 1:6)
set.seed(20210323)
rf.fit <- train(CRIM ~ . , 
                data_train, 
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl1)
# select optimal tuning parameter
#rf.fit$bestTune
rf.tun.plt = ggplot(rf.fit, highlight = TRUE)

# predicted y
rf.pred <- predict(rf.fit, newdata = data_test)
# test error
rf.mse = mean((rf.pred - y_test)^2)

# --- boosting
gbm.grid <- expand.grid(n.trees = c(1000,2000,3000), # B
                        interaction.depth = 1:10, # d
                        shrinkage = c(0.0008,0.001,0.0012), # lambda
                        n.minobsinnode = c(13,14,15,16)) # min # of obs in terminal nodes

set.seed(20210323)
gbm.fit <- train(CRIM ~ . , 
                 data_train,  
                 method = "gbm", # call gbm
                 tuneGrid = gbm.grid,
                 trControl = ctrl1,
                 verbose = FALSE)

# select optimal tuning parameter
#gbm.fit$bestTune
gbm.tun.plt = ggplot(gbm.fit, highlight = TRUE)

# predicted y
gbm.pred <- predict(gbm.fit, newdata = data_test)
# test error
gbm.mse = mean((gbm.pred - y_test)^2)
```

```{r 4.model_compare}
### Model comparison

resamp <- resamples(list(lm = lm.fit, 
                         ridge = ridge.fit, 
                         lasso = lasso.fit,
                         enet = enet.fit,
                         pcr = pcr.fit, 
                         pls = pls.fit,
                         gam = gam.fit,
                         mars = mars.fit,
                         rpart = rpart.fit,
                         ctree = ctree.fit,
                         rf = rf.fit,
                         gbm = gbm.fit))
train.rmse = summary(resamp)$values %>% select(matches("RMSE"))
colnames(train.rmse) = gsub("~RMSE","",colnames(train.rmse))

test_error = 
  data.frame(
    model = colnames(train.rmse), 
    RMSE = c(lm.mse,ridge.mse,lasso.mse,enet.mse,pcr.mse,pls.mse,NA,mars.mse,rpart.mse,ctree.mse,rf.mse,gbm.mse) %>% sqrt() %>% round(2), 
    test = "test"
  )

# plot rmse
model_compare = 
  train.rmse %>% 
  pivot_longer(1:12, names_to = "model", 
               values_to = "RMSE") %>% 
  ggplot(aes(y = reorder(model, RMSE, median), x = RMSE)) + 
  geom_boxplot(alpha = 0.3) + 
  stat_summary(fun = mean, geom = "point", 
               shape = 1, size = 2) +
  labs(y = NULL) + 
  theme_bw() + 
  theme(legend.position = "none")  + 
  geom_point(data = test_error, 
             aes(x = RMSE), color = "#d95f02", 
             shape = 20, size = 2) + 
  labs(caption = "Orange point: RMSE from test data")
```

There were one or few tuning parameters for each of the models except the linear regression model. Within the models that require tuning parameters, we specified a grid of numbers and then selected the one that gave the smallest cross-validation error. We then compared each of these models with optimal tuning parameters and decided the final model based on their training performance, i.e., cross-validation RMSE. Both median and mean RMSEs are reasonable evaluation metrics for the selection of the final model. We discussed both metrics and determined the appropriate final model, which would be used to predict the crime rate. 

## Model Tuning

```{r}
# summary(lm.fit$finalModel)
# 
# summary(ridge.fit$finalModel)
# ridge_coef = coef(ridge.fit$finalModel, ridge.fit$bestTune$lambda)
# ridge_coef_sum = sum(as.vector(ridge_coef) != 0)
# 
# summary(lasso.fit$finalModel)
# lasso_coef = coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)
# lasso_coef_sum = sum(as.vector(lasso_coef) != 0)
# 
# summary(enet.fit$finalModel)
# elastic_net_coef = coef(enet.fit$finalModel, enet.fit$bestTune$lambda)
# elastic_coef_sum = sum(as.vector(elastic_net_coef) != 0)
# 
# summary(pcr.fit$finalModel)
# summary(pls.fit$finalModel)
# summary(gam.fit$finalModel)
# coef(gam.fit$finalModel,gam.fit$bestTune$method)
# 
# summary(mars.fit$finalModel)
# 
# summary(rpart.fit$finalModel)
# coef(rpart.fit$finalModel,rpart.fit$bestTune$cp)
# 
# summary(ctree.fit$finalModel)
# summary(rf.fit$finalModel)
# summary(gbm.fit$finalModel)
```

(1) Linear Regression: Since Linear Model has a closed form solution, we did not need to tune parameters for this model. At 5% significance level, there are 5 out of 13 predictors that are significant, they are ZN, DIS, RAD, B, and MEDV. 

(2) Ridge Regression: We tuned 100 lambda values in between $e^2$ to $e^{-3}$. From Figure A1 in the Appendix, the lambda value with the smallest cross-validation RMSE is `r round(ridge.fit$bestTune$lambda,2)`, with a corresponding log value `r round(log(ridge.fit$bestTune$lambda),2)` on the plot. The Ridge model with the smallest cross-validation RMSE utilized all 13 predictors.

(3) Lasso Regression: We tuned 100 lambda values in between $e$ to $e^{-5}$. From Figure A2 in the Appendix, the best lambda value with the smallest cross-validation RMSE is `r round(lasso.fit$bestTune$lambda,2)`, with a corresponding log value `r round(log(lasso.fit$bestTune$lambda),2)` on the plot. The Lasso model with the smallest cross-validation RMSE utilized 4 predictors, which are RAD, B, LSTAT, and MEDV.

(4) Elastic Net Model: We tuned 5 alpha values in between 0 to 1, and tuned 50 lambda values in between $e^{-10}$ to $e^{-3}$. From Figure A3 in the Appendix we can see that alpha = 0 (ridge model) has significant lower cross-validation RMSE compared with other alpha values. The Elastic Net Model with the smallest cross-validation RMSE utilized all 13 predictors.

(5) Principle component regression (PCR) model: We tuned the number of components from 1 to 13. Figure A4 in the Appendix shows the optimal components number is 13.

(6) Partial Least Squares (PLS) Model: We tuned the number of components from 1 to 13 and Figure A5 in the Appendix shows the optimal components number is 10.

(7) Generalized Additive Model (GAM): The GAM model did not provide us the way to tune the parameters. The function automatically returned the most optimal model. The best GAM model used all 13 predictors and takes the form of crime rate = CHASotherwise + RAD + s(ZN) + s(PTRATIO) + s(TAX) + s(INDUS) + s(NOX) + s(MEDV) + s(B) + s(AGE) + s(DIS) + s(RM) + s(LSTAT). Specifically, ZN, PTRATIO, TAX, INDUS, NOX, and AGE have a linear association with crime rate whereas the rest of the predictors have a non-linear association. 

(8) Multivariate Adaptive Regression Splines (MARS): We tuned the maximum degree of interaction (degree) from 1 to 3 and the maximum number of terms (including intercept) in the pruned model (nprune) from 2 to 22. The best model with the smallest cross-validation RMSE is with nprune = 9 and degree = 2 (see Figure A6 in the Appendix). The best model selected 9 of 22 terms, and 6 of 13 predictors (DIS, RAD, MEDV, LSTAT, AGE, and NOX).

(9) Regression Tree Model: We tuned 50 values of Complexity Parameters (cp, the minimum improvement in the model needed at each node) from $e^{-6}$ to $e^{-3}$. The model with the smallest cross-validation RMSE is with cp = `r round(rpart.fit$bestTune,3)` (see Figure A7 in the Appendix). 

(10) Conditional Inference Tree Model: We tuned 50 values of the minimum criterion for a split (mincriterion) from $1 - e^{-5}$ to $1 - e^{-3}$. The model with the smallest cross-validation RMSE is with mincriterion = `r round(ctree.fit$bestTune,2)` (see Figure A8 in the Appendix). 

(11) Random Forest Model: We tuned the number of selected variable at each split from 1 to 13, and tuned the minimum node size from 1 to 6. The model with the smallest cross-validation RMSE is with 1 selected variable at each split and 4 minimum node size (see Figure A9 in the Appendix). 

(12) Boosting Model: We tuned the number of trees (n.trees) with values equal to 1000, 2000 and 3000, tuned the maximum depth of each tree (interaction.depth) with values from 1 to 10, tuned the shrinkage parameter with values equal to 0.0008, 0.001, and 0.0012, and tuned the minimum number of observations in terminal nodes (n.minobsinnode) from 13 to 16. The model with the smallest cross-validation RMSE is with n.trees = 2000, interaction.depth = 6, shrinkage = 0.001, and n.minobsinnode = 15 (see Figure A10 in the Appendix). 

## Model Summary

Figure 4 displays the distribution of the training errors (in black) of each of the models that we considered in this project. We can see that both random forest (rf) model and boosting (gbm) model performed well compared to the other models. Both models have similar performance using the training data; the random forest model has a slightly larger mean RMSE but a smaller median RMSE than the boosting model. As there is extreme value in RMSE, median RMSE can be a more appropriate criterion as it is insensitive to extreme values whereas mean is. Hence, we decided to use the random forest model, which has a smaller median RMSE, as our final model. 

We also evaluated the test performance of the models in the form of RMSE as displayed in Figure 4 (in orange). We can observe that among all the models, regression tree (rpart) model has the smallest test error, followed by boosting model (gbm), MARS model (mars), and random forest model (rf). Our selected final model, the random forest model, has its test error (RMSE) close to its training error (mean RMSE). This is desirable and it indicates that random forest model seems to have a consistently reliable predictive ability. Therefore, our decision of choosing random forest model looks reasonable and random forest seems to be flexible enough to make the prediction and able to successfully capture the non-linearity between the predictors and the crime rate. 

## Final Model Interpretation 

By the nature of random forest model, no specific assumption is made with regard to the functional relationship between the response variable and the predictor variables. As a black-box model, the random forest model does not have an explicit form. However, it can be interpreted, both globally and locally, using visualization tools. 

```{r 5.final_model_visualization,include=FALSE}
# Variable Importance Plot
set.seed(20210323)
vip_plot =
  vip(rf.fit,
      method = "permute", 
      train = data_train,
      target = "CRIM",
      metric = "RMSE",
      nsim = 100,
      pred_wrapper = predict,
      geom = "boxplot",
      all_permutations = TRUE,
      mapping = aes_string(fill = "Variable"),
      num_features = 13)

# Individual Conditional Expectation (ICE) curves
ice1.rf <- rf.fit %>% 
  partial(pred.var = "MEDV", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE)

ice2.rf <- rf.fit %>% 
  partial(pred.var = "DIS", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE)

ice3.rf <- rf.fit %>% 
  partial(pred.var = "RAD", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE)

ice4.rf <- rf.fit %>% 
  partial(pred.var = "TAX", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE) 

ice5.rf <- rf.fit %>% 
  partial(pred.var = "B", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE)

ice6.rf <- rf.fit %>% 
  partial(pred.var = "PTRATIO", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE)

ice7.rf <- rf.fit %>% 
  partial(pred.var = "AGE", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE)

ice8.rf <- rf.fit %>% 
  partial(pred.var = "NOX", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE)

ice9.rf <- rf.fit %>% 
  partial(pred.var = "LSTAT", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE)

ice10.rf <- rf.fit %>% 
  partial(pred.var = "INDUS", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE)

ice11.rf <- rf.fit %>% 
  partial(pred.var = "RM", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE)

ice12.rf <- rf.fit %>% 
  partial(pred.var = "ZN", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE)

ice13.rf <- rf.fit %>% 
  partial(pred.var = "CHAS", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = data_train, alpha = .1, 
           center = TRUE)
```

Based on the variable importance plot (VIP) in Figure 5A, MEDV, DIS, RAD, TAX, and B are the top 5 variables of importance in predicting the crime rate. The individual conditional expectation (ICE) curves (Figure 5B) illustrate how these variables impact the prediction of the response variable. For example, the crime rate seems to mildly fluctuate when the median value of owner-occupied homes (MEDV) is below 10 thousand dollars. It decreases quickly when the median value is between 10 and 12 thousand dollars, and decreases slowly when the median value is greater than 12 thousand dollars. When the median value exceeds 17 thousand dollars, the crime rate becomes stable. Similarly, we can see that the crime rate seems to first decrease with weighted distances to five Boston employment centers (DIS) and then remain stable after the weighted distances is around 1.9. Overall, the per capita crime rate seems to be stable when the index of accessibility to radial highways (RAD) is below 14. It starts to slowly increase after the index exceeds 14, and quickly jumps up when the index is around 16. However, when the index is greater than 16, the per capita crime rate seems to become stable again. Furthermore, we can see that the crime rate seems to be relatively stable when the full-value property-tax rate per $10,000 (TAX) is below 540, and it increases when TAX is between 540 and 570. After remaining stable when TAX is between 570 and 680, the crime rate decreases a little and then becomes stable again when TAX exceeds 690. In addition, the per capita crime rate seems to quickly decrease when the index of proportion of blacks by town (B) is below 20, and it slowly decreases when the index B is greater than 20.

```{r 5.lime,include=FALSE}
explainer.rf <- lime(data[trRows,-1], rf.fit) 

new_obs <- data[-trRows,-1][1:20,] #first 20 test x vars
explanation.obs <- explain(new_obs,
                           explainer.rf, 
                           n_features = 13)

#plot_features(explanation.obs)
plot_explanations(explanation.obs)
```

We can also visualize the result of our final model on new observations, to understand our final model more and to evaluate its performance. We used the first 20 test data observations for illustration. Based on the LIME plot (Figure 5C), we can see that the feature weight tends to be positive if the median value of owner-occupied homes (MEDV) is below its first quartile (Q1) 16.9 thousand dollars, but tends to be negative if MEDV is above 16.9 thousand dollars. This implies that lower MEDV is associated with higher crime rate. Similar pattern can also be observed for the index B, the average number of rooms (RM), and the proportion of residential land (ZN) -- lower B (meaning higher proportion of blacks) or lower RM or lower ZN is associated with higher crime rate, though the effects seem to be weaker. On the contrary, the feature weight of the index of accessibility to radial highways (RAD) tends to be negative if RAD is below 5, but tends to be positive if RAD is above 5; this indicates that higher RAD is associated with higher crime rate. Similarly, we can see that higher full-value property-tax rate (TAX), or higher pupil-teacher ratio (PTRATIO), or higher percentage of lower status of the population (LSTAT) is associated with higher crime rate. All these interpretations are reasonable; therefore, we considered our final model to be reliable. 



# Discussion 

We did not perform transformations on the predictors due to the concern of interpretability. Though transformation might compromise the interpretability, it could possibly improve model performance. Future research could consider applying transformation on predictors. 

Overall, our model suggests that the median value of owner-occupied homes, distances to employment centers, and accessibility to radial highways are the top 3 variables that play an important role in predicting the crime rate. Specifically, the crime rate is predicted to decrease with higher median home value, to decrease if the distance to employment centers is farther, and to increase with the accessibility to radial highways when the index of latter exceeds 14. These findings are consistent with our natural understanding and expectation and provide meaningful insight into public safety. For example, communities with higher median home values may have better security facilities, which leads to lower crime rate; longer distance to employment centers may discourage the criminals of those centers from traveling to the communities; better accessibility to radial highways would allow the criminals to flee more easily, which could be a potential incentive for crimes. Therefore, the policymakers or authorities might consider making more effort in these forehead mentioned areas to reduce the crime rate and protect the overall population.


\newpage

# Tables and Figures

__Table 1.__ Variable Description
```{r var_descpt}
var_descpt
```

\pagebreak

```{r var_distn, fig.height= 7,fig.width= 9}
var_distn
```
\begin{center}
$\bf{Figure \ 1.}$ \ Variable Distribution (Training Dataset)
\end{center}

\pagebreak

```{r ctns_var_vs_crime, fig.height= 9, fig.width= 8}
library(cowplot)
figure <- plot_grid(scat_plt, box_plt, align = "v", nrow = 2, rel_heights = c(2/3, 1/3))
figure
```
\begin{center}
$\bf{Figure \ 2.}$ \ Variables vs. Crime Rate (Training Dataset)
\end{center}

\pagebreak

```{r cor_plt}
corrplot(cor(data0[trRows,]), method = "circle", type = "full", tl.cex = 0.53)
```
\begin{center}
$\bf{Figure \ 3.}$ \ Correlation Plot (Training Dataset)
\end{center}

\pagebreak

```{r model_compare}
model_compare
```
\begin{center}
$\bf{Figure \ 4.}$ \ Predictive Performance of All Models
\end{center}

\pagebreak

```{r vip_plt}
vip_plot
```
\begin{center}
$\bf{Figure \ 5A.}$ \ The Variable Importance Plot (VIP) from Random Forest Model (Final Model)
\end{center}

\pagebreak

```{r ice_curve}
grid.arrange(ice1.rf, ice2.rf, ice3.rf, ice4.rf, ice5.rf, ice6.rf, ice7.rf,
             ice8.rf, ice9.rf, ice10.rf, ice11.rf, ice12.rf, ice13.rf, nrow = 4)
```
\begin{center}
$\bf{Figure \ 5B.}$ \ The Individual Conditional Expectation (ICE) Curves from Random Forest Model (Final Model)
\end{center}

\pagebreak

```{r lime_plt, fig.height= 9, fig.width= 8}
plot_explanations(explanation.obs)
```
\begin{center}
$\bf{Figure \ 5C.}$ \ The Local Interpretable Model-agnostic Explanations (LIME) Plot from Random Forest Model (Final Model)
\end{center}



\newpage

# Appendix of Model Tuning

## Ridge Model

```{r}
# --- ridge 
# ridge.fit$bestTune$lambda
ridge.tun.plt = plot(ridge.fit, xTrans = log); ridge.tun.plt
```
\begin{center}
$\bf{Figure \ A1.}$ \ Tuning Lambda of Ridge Model
\end{center}

\pagebreak

## Lasso Model
 
```{r}
# --- lasso
#lasso.fit$bestTune$lambda
lasso.tun.plt = plot(lasso.fit, xTrans = log); lasso.tun.plt
```
\begin{center}
$\bf{Figure \ A2.}$ \ Tuning Lambda of Lasso Model
\end{center}

\pagebreak

## Elastic Net Model

```{r}
# --- Elastic net
#enet.fit$bestTune
emet.tun.plt = ggplot(enet.fit); emet.tun.plt
```
\begin{center}
$\bf{Figure \ A3.}$ \ Tuning Alpha and Lambda of Elastic Net Model
\end{center}

\pagebreak

## PCR Model

```{r}
# --- pcr
pcr.tun.plt = ggplot(pcr.fit, highlight = TRUE); pcr.tun.plt
```
\begin{center}
$\bf{Figure \ A4.}$ \ Tuning Optimal Components of PCR Model
\end{center}

\pagebreak

## PLS Model

```{r}
# --- pls
pls.tun.plt = ggplot(pls.fit, highlight = TRUE); pls.tun.plt
```
\begin{center}
$\bf{Figure \ A5.}$ \ Tuning Optimal Components of PLS Model 
\end{center}

\pagebreak

## MARS Model

```{r}
# --- MARS model
#mars.fit$bestTune
mars.tun.plt = ggplot(mars.fit); mars.tun.plt
```
\begin{center}
$\bf{Figure \ A6.}$ \ Tuning MARS Model
\end{center}

\pagebreak

## Regression Tree Model

```{r}
# --- regression tree
#rpart.fit$bestTune
rpart.tun.plt = ggplot(rpart.fit, highlight = TRUE); rpart.tun.plt
```
\begin{center}
$\bf{Figure \ A7.}$ \ Tuning Complexity Parameter of Regression Tree Model 
\end{center}

\newpage

## Conditional Inference Tree Model

```{r}
# --- conditional inference tree model
ctree.tun.plt = ggplot(ctree.fit, highlight = TRUE); ctree.tun.plt
# ctree.fit$bestTune
```
\begin{center}
$\bf{Figure \ A8.}$ \ Tuning Mincriterion of Conditional Inference Tree Model
\end{center}

\pagebreak

## Random Forest Model

```{r}
# --- random forest (bagging at mtry = 13)
rf.tun.plt = ggplot(rf.fit, highlight = TRUE); rf.tun.plt
# rf.fit$bestTune
```
\begin{center}
$\bf{Figure \ A9.}$ \ Tuning Random Forest Model
\end{center}

\pagebreak

## Boosting Model

```{r}
# --- boosting
gbm.tun.plt = ggplot(gbm.fit, highlight = TRUE); gbm.tun.plt
# gbm.fit$bestTune
```
\begin{center}
$\bf{Figure \ A10.}$ \ Tuning Boosting Model 
\end{center}



